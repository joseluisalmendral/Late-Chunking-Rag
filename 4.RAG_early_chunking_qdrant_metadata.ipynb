{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG con early chunking y Qdrant intentando adjuntar metadata para tener mayor contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/joseluis.fernandez/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "from PyPDF2 import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import uuid\n",
    "\n",
    "# Cliente de Qdrant y modelos\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "\n",
    "# ------------------------------\n",
    "# 1. CONFIGURACIÓN INICIAL\n",
    "# ------------------------------\n",
    "\n",
    "# Descargar recursos necesarios de NLTK\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Cargar variables de entorno (.env debe contener OPENAI_API_KEY, QDRANT_URL, QDRANT_API_KEY, etc.)\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Cargar el modelo de embeddings (por ejemplo, all-MiniLM-L6-v2)\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embedding_dim = 384  # Dimensión del embedding para este modelo\n",
    "\n",
    "# Parámetros para el chunking\n",
    "max_chunk_tokens = 600\n",
    "overlap_tokens = 80\n",
    "\n",
    "# Archivo local para guardar la información de PDFs indexados\n",
    "INDEXED_FILES_PATH = \"indexed_files.json\"\n",
    "\n",
    "# ------------------------------\n",
    "# 2. CONEXIÓN A QDRANT CLOUD\n",
    "# ------------------------------\n",
    "\n",
    "qdrant_url = os.getenv(\"QDRANT_URL\")         \n",
    "qdrant_api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    "qdrant = QdrantClient(url=qdrant_url, api_key=qdrant_api_key)\n",
    "collection_name = \"pdf_chunks\"\n",
    "\n",
    "# Crear la colección en Qdrant si no existe\n",
    "try:\n",
    "    qdrant.get_collection(collection_name=collection_name)\n",
    "    print('La colección de Qdrant ya existe.')\n",
    "except Exception as e:\n",
    "    print(\"Creando colección en Qdrant Cloud...\")\n",
    "    qdrant.recreate_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=models.VectorParams(size=embedding_dim, distance=models.Distance.COSINE)\n",
    "    )\n",
    "\n",
    "# ------------------------------\n",
    "# 3. FUNCIÓN DE CHUNKING (EARLY CHUNKING)\n",
    "# ------------------------------\n",
    "\n",
    "def late_chunking(text, max_chunk_tokens=max_chunk_tokens, overlap_tokens=overlap_tokens):\n",
    "    \"\"\"\n",
    "    Fragmenta el texto en chunks de hasta 'max_chunk_tokens' tokens,\n",
    "    agregando un solapamiento de 'overlap_tokens' entre fragmentos.\n",
    "    Se utiliza el tokenizador de oraciones de NLTK para evitar cortar oraciones.\n",
    "    \"\"\"\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_tokens = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = sentence.split()\n",
    "        num_tokens = len(sentence_tokens)\n",
    "        # Si se supera el límite al agregar la oración, se guarda el chunk actual\n",
    "        if current_tokens + num_tokens > max_chunk_tokens:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            # Mantener solapamiento: conservar los últimos 'overlap_tokens' tokens\n",
    "            current_chunk_tokens = current_chunk.split()\n",
    "            current_chunk = \" \".join(current_chunk_tokens[-overlap_tokens:]) if overlap_tokens > 0 else \"\"\n",
    "            current_tokens = len(current_chunk.split())\n",
    "        current_chunk += \" \" + sentence\n",
    "        current_tokens += num_tokens\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "# ------------------------------\n",
    "# 4. CARGAR Y PROCESAR PDFs CON EARLY CHUNKING (Solo nuevos/modificados)\n",
    "# ------------------------------\n",
    "\n",
    "def load_indexed_files(cache_path):\n",
    "    \"\"\"Carga el diccionario de PDFs ya indexados desde un archivo JSON.\"\"\"\n",
    "    if os.path.exists(cache_path):\n",
    "        try:\n",
    "            with open(cache_path, \"r\") as f:\n",
    "                return json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"Error al cargar el archivo de cache: {e}\")\n",
    "    return {}\n",
    "\n",
    "def save_indexed_files(cache_path, indexed_files):\n",
    "    \"\"\"Guarda el diccionario de PDFs indexados en un archivo JSON.\"\"\"\n",
    "    with open(cache_path, \"w\") as f:\n",
    "        json.dump(indexed_files, f)\n",
    "\n",
    "def load_and_chunk_pdfs(root_folder, indexed_files):\n",
    "    \"\"\"\n",
    "    Recorre recursivamente 'root_folder' para extraer el texto de cada PDF.\n",
    "    Si el PDF ya fue procesado (según su ruta y marca de tiempo), se omite.\n",
    "    Divide el contenido en chunks y añade metadata (bloque, nombre del archivo, ruta y mod_time).\n",
    "    \n",
    "    Retorna:\n",
    "      - Una lista de diccionarios con keys: 'text' y 'metadata'.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                mod_time = os.path.getmtime(file_path)\n",
    "                # Verificar si el archivo ya fue indexado y no se ha modificado\n",
    "                if file_path in indexed_files and indexed_files[file_path] == mod_time:\n",
    "                    continue  # Se salta este archivo, ya fue indexado\n",
    "                block_name = os.path.basename(subdir)\n",
    "                try:\n",
    "                    reader = PdfReader(file_path)\n",
    "                    full_text = \"\"\n",
    "                    for page in reader.pages:\n",
    "                        page_text = page.extract_text()\n",
    "                        if page_text:\n",
    "                            full_text += page_text + \"\\n\"\n",
    "                    # Aplicar chunking al texto completo\n",
    "                    pdf_chunks = late_chunking(full_text)\n",
    "                    for chunk in pdf_chunks:\n",
    "                        # Incluir metadata en cada chunk para proporcionar contexto al LLM\n",
    "                        chunk_text = f\"[Bloque: {block_name} | PDF: {file}]\\n{chunk}\"\n",
    "                        chunks.append({\n",
    "                            \"text\": chunk_text,\n",
    "                            \"metadata\": {\n",
    "                                \"block\": block_name,\n",
    "                                \"file\": file,\n",
    "                                \"path\": file_path,\n",
    "                                \"mod_time\": mod_time\n",
    "                            }\n",
    "                        })\n",
    "                    # Actualizar el diccionario de archivos indexados\n",
    "                    indexed_files[file_path] = mod_time\n",
    "                except Exception as e:\n",
    "                    print(f\"Error al leer {file_path}: {e}\")\n",
    "    return chunks\n",
    "\n",
    "# Ruta de la carpeta que contiene los PDFs (ajusta según tus necesidades)\n",
    "pdf_root = \"temario_opos\"\n",
    "\n",
    "# Cargar la cache de PDFs indexados\n",
    "indexed_files = load_indexed_files(INDEXED_FILES_PATH)\n",
    "\n",
    "# Procesar solo PDFs nuevos o modificados\n",
    "new_chunks = load_and_chunk_pdfs(pdf_root, indexed_files)\n",
    "if new_chunks:\n",
    "    print(f\"Se han generado {len(new_chunks)} nuevos chunks desde '{pdf_root}'.\")\n",
    "else:\n",
    "    print(\"No se encontraron PDFs nuevos o modificados. Se utilizará el índice existente en Qdrant.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 5. INDEXACIÓN DE CHUNKS NUEVOS EN QDRANT CLOUD (si hay)\n",
    "# ------------------------------\n",
    "\n",
    "if new_chunks:\n",
    "    points = []\n",
    "    for chunk in new_chunks:\n",
    "        # Calcular embedding del chunk\n",
    "        embedding = embedding_model.encode(chunk[\"text\"], convert_to_numpy=True)\n",
    "        # Crear un punto para Qdrant (usando un id único)\n",
    "        point = models.PointStruct(\n",
    "            id=str(uuid.uuid4()),\n",
    "            vector=embedding.tolist(),\n",
    "            payload={\n",
    "                \"text\": chunk[\"text\"],\n",
    "                \"block\": chunk[\"metadata\"][\"block\"],\n",
    "                \"file\": chunk[\"metadata\"][\"file\"],\n",
    "                \"path\": chunk[\"metadata\"][\"path\"],\n",
    "                \"mod_time\": chunk[\"metadata\"][\"mod_time\"]\n",
    "            }\n",
    "        )\n",
    "        points.append(point)\n",
    "    \n",
    "    print(\"Indexando nuevos chunks en Qdrant Cloud...\")\n",
    "    qdrant.upsert(collection_name=collection_name, points=points)\n",
    "    print(\"Indexación de nuevos chunks completada.\")\n",
    "    # Actualizar la cache local de archivos indexados\n",
    "    save_indexed_files(INDEXED_FILES_PATH, indexed_files)\n",
    "\n",
    "# ------------------------------\n",
    "# 6. FUNCIÓN PARA RESPONDER CONSULTAS (RAG)\n",
    "# ------------------------------\n",
    "\n",
    "def answer_query(query, top_k=5, token_budget=2000):\n",
    "    \"\"\"\n",
    "    Procesa la consulta del usuario:\n",
    "      - Genera el embedding de la consulta.\n",
    "      - Realiza una búsqueda en Qdrant Cloud para recuperar los chunks más relevantes.\n",
    "      - Combina los chunks hasta alcanzar un presupuesto de tokens y forma el contexto.\n",
    "      - Consulta la API de OpenAI para generar la respuesta basada en el contexto.\n",
    "    \"\"\"\n",
    "    # Generar embedding para la consulta\n",
    "    query_embedding = embedding_model.encode(query, convert_to_numpy=True)\n",
    "    \n",
    "    # Buscar los chunks más relevantes en Qdrant Cloud\n",
    "    search_result = qdrant.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding.tolist(),\n",
    "        limit=top_k,\n",
    "        with_payload=True\n",
    "    )\n",
    "    \n",
    "    # Seleccionar y combinar chunks según el token_budget\n",
    "    selected_chunks = []\n",
    "    total_tokens = 0\n",
    "    for res in search_result:\n",
    "        chunk_text = res.payload.get('text', '')\n",
    "        tokens = len(chunk_text.split())\n",
    "        if total_tokens + tokens <= token_budget:\n",
    "            selected_chunks.append(chunk_text)\n",
    "            total_tokens += tokens\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    context = \"\\n\\n\".join(selected_chunks)\n",
    "    prompt = f\"Contexto:\\n{context}\\n\\nPregunta: {query}\"\n",
    "    \n",
    "    # Llamar a la API de ChatCompletion\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Eres un asistente educativo experto. Responde solo en base al contexto que te proporciono.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# ------------------------------\n",
    "# 7. BUCLE DE INTERACCIÓN CONTINUA\n",
    "# ------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Bienvenido al chatbot. Escribe 'salir' o 'exit' para terminar la conversación.\")\n",
    "    while True:\n",
    "        consulta = input(\"Introduce tu consulta: \")\n",
    "        if consulta.lower() in [\"salir\", \"exit\"]:\n",
    "            print(\"Finalizando la conversación. ¡Hasta luego!\")\n",
    "            break\n",
    "        respuesta = answer_query(consulta)\n",
    "        print(\"\\nRespuesta del chatbot:\")\n",
    "        print(respuesta)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
