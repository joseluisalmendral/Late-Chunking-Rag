{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG implementando método del paper (https://arxiv.org/html/2409.04701v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/joseluis.fernandez/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se han cargado 67 documentos desde 'temario_opos'.\n",
      "Bienvenido al chatbot. Escribe 'salir' o 'exit' para terminar la conversación.\n",
      "\n",
      "Respuesta del chatbot:\n",
      "Lo siento, pero no puedo responder preguntas que no estén relacionadas con oposiciones. Si tienes alguna consulta sobre temas de oposiciones, estaré encantado de ayudarte.\n",
      "\n",
      "Respuesta del chatbot:\n",
      "En el temario que has proporcionado, se pueden identificar al menos seis bloques principales. Estos bloques son:\n",
      "\n",
      "1. **Bloque I**: La Constitución Española de 1978: estructura y contenido. La reforma de la Constitución.\n",
      "2. **Bloque II**: Organización territorial (I): las Comunidades Autónomas. Los Estatutos de Autonomía.\n",
      "3. **Bloque III**: Otras políticas públicas.\n",
      "4. **Bloque IV**: Sistema sanitario: distribución de competencias, gestión y financiación.\n",
      "5. **Bloque V**: Acción Exterior.\n",
      "6. **Bloque VI**: Pagos a justificar. Anticipos de caja.\n",
      "\n",
      "Si necesitas más detalles sobre cada bloque o su contenido, no dudes en preguntar.\n",
      "\n",
      "Respuesta del chatbot:\n",
      "El Bloque V trata sobre la Administración de recursos humanos. Este bloque aborda temas relacionados con la gestión del personal en las administraciones públicas, incluyendo aspectos como la selección, formación, evaluación y desarrollo del personal, así como la normativa que regula estas áreas. Es fundamental para entender cómo se organiza y gestiona el capital humano en el ámbito público.\n",
      "\n",
      "Respuesta del chatbot:\n",
      "El régimen de jornada de trabajo, permisos y vacaciones del personal laboral al servicio de las Administraciones Públicas se rige por lo establecido en el capítulo correspondiente del Texto Refundido de la Ley del Estatuto Básico del Empleado Público (TREBEP) y por la legislación laboral correspondiente. Esto implica que se aplicarán las normativas específicas que regulan estos aspectos en el ámbito laboral, así como las disposiciones que se establezcan en los convenios colectivos aplicables.\n",
      "Finalizando la conversación. ¡Hasta luego!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import openai\n",
    "import nltk\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# ------------------------------\n",
    "# 1. CONFIGURACIÓN INICIAL\n",
    "# ------------------------------\n",
    "\n",
    "# Cargar variables de entorno (asegúrate de tener un archivo .env con OPENAI_API_KEY)\n",
    "load_dotenv()\n",
    "\n",
    "# Descargar los recursos necesarios de NLTK para tokenización\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Configurar la API key de OpenAI\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Cargar el modelo de embeddings\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Parámetros de chunking a nivel local\n",
    "max_chunk_tokens_ = 600\n",
    "overlap_tokens_ = 80\n",
    "\n",
    "# ------------------------------\n",
    "# 2. FUNCIÓN PARA CARGAR PDFs\n",
    "# ------------------------------\n",
    "\n",
    "def load_pdf_texts(root_folder):\n",
    "    \"\"\"\n",
    "    Recorre recursivamente el directorio 'root_folder' y extrae el texto de cada PDF.\n",
    "    Se agrega metadata (nombre del bloque y del archivo) al inicio del texto.\n",
    "    \n",
    "    Retorna:\n",
    "      - documents: lista de textos de cada PDF (con metadata incluida).\n",
    "      - metadata: lista de diccionarios con información del bloque, nombre del archivo y ruta.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    metadata = []\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                try:\n",
    "                    reader = PdfReader(file_path)\n",
    "                    text = \"\"\n",
    "                    for page in reader.pages:\n",
    "                        page_text = page.extract_text()\n",
    "                        if page_text:\n",
    "                            text += page_text + \"\\n\"\n",
    "                    # Suponemos que el nombre del bloque es el nombre del directorio inmediato donde se encuentra el PDF\n",
    "                    block_name = os.path.basename(subdir)\n",
    "                    # Incluir metadata en el texto para que el LLM sepa el origen\n",
    "                    document_text = f\"[Bloque: {block_name} | PDF: {file}]\\n{text}\"\n",
    "                    documents.append(document_text)\n",
    "                    metadata.append({\"block\": block_name, \"file\": file, \"path\": file_path})\n",
    "                except Exception as e:\n",
    "                    print(f\"Error al leer {file_path}: {e}\")\n",
    "    return documents, metadata\n",
    "\n",
    "# Ruta de la carpeta que contiene los bloques (modifica esta ruta según tus necesidades)\n",
    "pdf_root = \"temario_opos\"\n",
    "documents, doc_metadata = load_pdf_texts(pdf_root)\n",
    "print(f\"Se han cargado {len(documents)} documentos desde '{pdf_root}'.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 3. CREACIÓN DEL ÍNDICE FAISS\n",
    "# ------------------------------\n",
    "\n",
    "# Generar embeddings para cada documento completo\n",
    "document_embeddings = embedding_model.encode(documents, convert_to_numpy=True)\n",
    "\n",
    "# Crear el índice FAISS usando la distancia L2\n",
    "dimension = document_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(document_embeddings)\n",
    "\n",
    "# ------------------------------\n",
    "# 4. FUNCIÓN DE LATE CHUNKING\n",
    "# ------------------------------\n",
    "\n",
    "def late_chunking(text, max_chunk_tokens=600, overlap_tokens=80):\n",
    "    \"\"\"\n",
    "    Fragmenta el texto en chunks de hasta 'max_chunk_tokens' tokens,\n",
    "    agregando un solapamiento de 'overlap_tokens' tokens entre fragmentos.\n",
    "    \n",
    "    Se utiliza el tokenizador de oraciones de NLTK para no cortar oraciones a la mitad.\n",
    "    \"\"\"\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_tokens = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Contamos tokens de forma aproximada separando por espacios\n",
    "        sentence_tokens = sentence.split()\n",
    "        num_tokens = len(sentence_tokens)\n",
    "        \n",
    "        # Si al agregar la oración se supera el límite, se guarda el chunk actual\n",
    "        if current_tokens + num_tokens > max_chunk_tokens:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            # Solapamiento: conservar los últimos 'overlap_tokens' tokens del chunk anterior\n",
    "            current_chunk_tokens = current_chunk.split()\n",
    "            current_chunk = \" \".join(current_chunk_tokens[-overlap_tokens:]) if overlap_tokens > 0 else \"\"\n",
    "            current_tokens = len(current_chunk.split())\n",
    "        \n",
    "        current_chunk += \" \" + sentence\n",
    "        current_tokens += num_tokens\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "# ------------------------------\n",
    "# 5. FUNCIÓN PARA EXTRAER Y RANQUEAR CHUNKS RELEVANTES\n",
    "# ------------------------------\n",
    "\n",
    "def get_relevant_chunks(query_embedding, retrieved_docs, \n",
    "                        chunk_threshold=300, max_chunk_tokens=600, overlap_tokens=80):\n",
    "    \"\"\"\n",
    "    Para cada documento recuperado, si es muy largo se aplica late chunking.\n",
    "    Se vuelve a calcular el embedding para cada chunk y se obtiene una puntuación\n",
    "    de similitud coseno con la consulta.\n",
    "    \n",
    "    Retorna una lista de diccionarios con keys: 'text', 'meta' y 'similarity'.\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "    # Para cada documento recuperado\n",
    "    for doc_text, meta in retrieved_docs:\n",
    "        # Si el documento es extenso, se fragmenta; de lo contrario, se usa completo\n",
    "        if len(doc_text.split()) > chunk_threshold:\n",
    "            chunks = late_chunking(doc_text, max_chunk_tokens=max_chunk_tokens, overlap_tokens=overlap_tokens)\n",
    "        else:\n",
    "            chunks = [doc_text]\n",
    "        \n",
    "        # Calcular la similitud de cada chunk con la consulta\n",
    "        for chunk in chunks:\n",
    "            chunk_embedding = embedding_model.encode(chunk, convert_to_numpy=True)\n",
    "            # Asegurarse de trabajar con vectores unidimensionales\n",
    "            q_emb = query_embedding.flatten()\n",
    "            c_emb = chunk_embedding.flatten()\n",
    "            # Calcular similitud coseno (se agrega un pequeño epsilon para evitar división por cero)\n",
    "            similarity = np.dot(q_emb, c_emb) / (np.linalg.norm(q_emb) * np.linalg.norm(c_emb) + 1e-10)\n",
    "            all_chunks.append({\"text\": chunk, \"meta\": meta, \"similarity\": similarity})\n",
    "    \n",
    "    # Ordenar todos los chunks de mayor a menor similitud\n",
    "    all_chunks.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
    "    return all_chunks\n",
    "\n",
    "# ------------------------------\n",
    "# 6. FUNCIÓN PARA RESPONDER CONSULTAS\n",
    "# ------------------------------\n",
    "\n",
    "def answer_query(query, top_k=4, token_budget=15000):\n",
    "    \"\"\"\n",
    "    Procesa la consulta del usuario:\n",
    "      - Se genera el embedding de la consulta.\n",
    "      - Se recuperan los top_k documentos completos usando FAISS.\n",
    "      - Se aplica late chunking a cada documento recuperado y se recalcula la similitud\n",
    "        de cada fragmento respecto a la consulta.\n",
    "      - Se seleccionan los fragmentos con mayor similitud hasta no superar el presupuesto de tokens.\n",
    "      - Se prepara el prompt combinando el contexto y la pregunta, y se consulta a la API de OpenAI.\n",
    "    \"\"\"\n",
    "    # Generar embedding para la consulta (nota: encode espera una lista)\n",
    "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)\n",
    "    \n",
    "    # Recuperar los top_k documentos relevantes mediante FAISS\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    retrieved_docs = []\n",
    "    for i in indices[0]:\n",
    "        doc_text = documents[i]\n",
    "        meta = doc_metadata[i]\n",
    "        retrieved_docs.append((doc_text, meta))\n",
    "    \n",
    "    # Aplicar late chunking y re-ranquear los fragments por similitud\n",
    "    all_chunks = get_relevant_chunks(query_embedding, retrieved_docs)\n",
    "    \n",
    "    # Seleccionar fragmentos hasta no superar el presupuesto de tokens para el contexto\n",
    "    selected_chunks = []\n",
    "    total_tokens = 0\n",
    "    for chunk_info in all_chunks:\n",
    "        chunk_text = chunk_info[\"text\"]\n",
    "        # Conteo aproximado de tokens (por espacio)\n",
    "        tokens = len(chunk_text.split())\n",
    "        if total_tokens + tokens <= token_budget:\n",
    "            selected_chunks.append(chunk_text)\n",
    "            total_tokens += tokens\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Combinar los fragmentos seleccionados para formar el contexto\n",
    "    context = \"\\n\\n\".join(selected_chunks)\n",
    "    \n",
    "    # Preparar el prompt para el modelo de lenguaje\n",
    "    prompt = f\"Contexto:\\n{context}\\n\\nPregunta: {query}\\nRespuesta:\"\n",
    "    \n",
    "    # Llamar a la API de ChatCompletion (modelo: gpt-4o-mini)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Eres un asistente educativo experto. Solo puedes responder a temas relacionados con las oposiciones.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# ------------------------------\n",
    "# 7. BUCLE DE INTERACCIÓN CONTINUA\n",
    "# ------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Bienvenido al chatbot. Escribe 'salir' o 'exit' para terminar la conversación.\")\n",
    "    while True:\n",
    "        consulta = input(\"Introduce tu consulta: \")\n",
    "        if consulta.lower() in [\"salir\", \"exit\"]:\n",
    "            print(\"Finalizando la conversación. ¡Hasta luego!\")\n",
    "            break\n",
    "        respuesta = answer_query(consulta)\n",
    "        print(\"\\nRespuesta del chatbot:\")\n",
    "        print(respuesta)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
